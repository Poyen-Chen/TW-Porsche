{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10454013,"sourceType":"datasetVersion","datasetId":6413817}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"widgets":{"application/vnd.jupyter.widget-state+json":{"c65c761364a64f8d9b387999a2fe9234":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9dbee88ce7e7411aa3c75fcbae8fd657","IPY_MODEL_a365f0f61c224a189e89a711328d06be","IPY_MODEL_77314fd9e9664d01bae0684311f04682"],"layout":"IPY_MODEL_1f39ad7d421a412db0a35471aaab9f31"}},"9dbee88ce7e7411aa3c75fcbae8fd657":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d5c66c125d384526b517cd35144a79a2","placeholder":"​","style":"IPY_MODEL_b171bd2024884b7e97d61bb70d200b98","value":"Generating train split: "}},"a365f0f61c224a189e89a711328d06be":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ba9d95dc32a149f4b3b942410c7bb134","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1365760bf0914b9fb613c122566a157e","value":1}},"77314fd9e9664d01bae0684311f04682":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_731b45ec695f4eaaa2d917e3817ef3a7","placeholder":"​","style":"IPY_MODEL_3292e64d80d64a999833f7b8d3ed04be","value":" 1652/0 [00:00&lt;00:00, 7687.72 examples/s]"}},"1f39ad7d421a412db0a35471aaab9f31":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d5c66c125d384526b517cd35144a79a2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b171bd2024884b7e97d61bb70d200b98":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ba9d95dc32a149f4b3b942410c7bb134":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"1365760bf0914b9fb613c122566a157e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"731b45ec695f4eaaa2d917e3817ef3a7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3292e64d80d64a999833f7b8d3ed04be":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"iUyqX2SE_E_C","cell_type":"code","source":"!pip install -U sentence_transformers\n!pip install xformers\n!pip install bitsandbytes\n!pip install peft\n!pip install huggingface_hub\n!pip install datasets","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-01-14T11:48:13.096023Z","iopub.execute_input":"2025-01-14T11:48:13.096233Z","iopub.status.idle":"2025-01-14T11:51:39.179613Z","shell.execute_reply.started":"2025-01-14T11:48:13.096213Z","shell.execute_reply":"2025-01-14T11:51:39.178707Z"},"id":"iUyqX2SE_E_C","outputId":"a2bd7574-8d66-451c-807b-82d9b65fadb4","trusted":true},"outputs":[{"name":"stdout","text":"Collecting sentence_transformers\n  Downloading sentence_transformers-3.3.1-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.44.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.5)\nRequirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.4.1+cu121)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.13.1)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.24.7)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (10.4.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.16.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.6.1)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (1.26.4)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.9.11)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.19.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\nDownloading sentence_transformers-3.3.1-py3-none-any.whl (268 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: sentence_transformers\nSuccessfully installed sentence_transformers-3.3.1\nCollecting xformers\n  Downloading xformers-0.0.29.post1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xformers) (1.26.4)\nCollecting torch==2.5.1 (from xformers)\n  Downloading torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl.metadata (28 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->xformers) (3.16.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->xformers) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->xformers) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->xformers) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->xformers) (2024.6.1)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.5.1->xformers)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.5.1->xformers)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.5.1->xformers)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.5.1->xformers)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.5.1->xformers)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.5.1->xformers)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch==2.5.1->xformers)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.5.1->xformers)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.5.1->xformers)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nccl-cu12==2.21.5 (from torch==2.5.1->xformers)\n  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvtx-cu12==12.4.127 (from torch==2.5.1->xformers)\n  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.5.1->xformers)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting triton==3.1.0 (from torch==2.5.1->xformers)\n  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\nCollecting sympy==1.13.1 (from torch==2.5.1->xformers)\n  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch==2.5.1->xformers) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.5.1->xformers) (2.1.5)\nDownloading xformers-0.0.29.post1-cp310-cp310-manylinux_2_28_x86_64.whl (15.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl (906.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.4/906.4 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: triton, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, xformers\n  Attempting uninstall: sympy\n    Found existing installation: sympy 1.13.3\n    Uninstalling sympy-1.13.3:\n      Successfully uninstalled sympy-1.13.3\n  Attempting uninstall: nvidia-nccl-cu12\n    Found existing installation: nvidia-nccl-cu12 2.23.4\n    Uninstalling nvidia-nccl-cu12-2.23.4:\n      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n  Attempting uninstall: torch\n    Found existing installation: torch 2.4.1+cu121\n    Uninstalling torch-2.4.1+cu121:\n      Successfully uninstalled torch-2.4.1+cu121\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nfastai 2.7.17 requires torch<2.5,>=1.10, but you have torch 2.5.1 which is incompatible.\ntorchaudio 2.4.1+cu121 requires torch==2.4.1, but you have torch 2.5.1 which is incompatible.\ntorchvision 0.19.1+cu121 requires torch==2.4.1, but you have torch 2.5.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 sympy-1.13.1 torch-2.5.1 triton-3.1.0 xformers-0.0.29.post1\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl.metadata (2.9 kB)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.5.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: typing_extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (4.12.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.16.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2024.6.1)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.3.1.170)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.4.127)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->bitsandbytes) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (2.1.5)\nDownloading bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl (69.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.45.0\nCollecting peft\n  Downloading peft-0.14.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (24.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.2)\nRequirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.5.1)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.44.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.66.5)\nRequirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.34.2)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.5)\nCollecting huggingface-hub>=0.25.0 (from peft)\n  Downloading huggingface_hub-0.27.1-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.0->peft) (3.16.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.0->peft) (2024.6.1)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.0->peft) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.3.1.170)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2024.9.11)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.19.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (2024.8.30)\nDownloading peft-0.14.0-py3-none-any.whl (374 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.8/374.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading huggingface_hub-0.27.1-py3-none-any.whl (450 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m450.7/450.7 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: huggingface-hub, peft\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 0.24.7\n    Uninstalling huggingface-hub-0.24.7:\n      Successfully uninstalled huggingface-hub-0.24.7\nSuccessfully installed huggingface-hub-0.27.1 peft-0.14.0\nRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.27.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.16.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.6.1)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.5)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.8.30)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (18.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\nRequirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.27.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.11.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\nRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","output_type":"stream"}],"execution_count":1},{"id":"fa0d7756-0d6b-456d-b337-a757d41e16a3","cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T12:17:41.175720Z","iopub.execute_input":"2025-01-14T12:17:41.176128Z","iopub.status.idle":"2025-01-14T12:17:41.180425Z","shell.execute_reply.started":"2025-01-14T12:17:41.176098Z","shell.execute_reply":"2025-01-14T12:17:41.179370Z"}},"outputs":[],"execution_count":26},{"id":"3c082570-e6e2-4a6b-b2e4-77e28ff4e54b","cell_type":"code","source":"import os\nimport json\nfrom sentence_transformers import SentenceTransformer\nimport torch\nfrom transformers import AutoModel, AutoTokenizer\nfrom safetensors.torch import load_file\nfrom peft import PeftModel\nfrom sklearn.preprocessing import normalize\nfrom datasets import load_dataset, concatenate_datasets, Dataset\nfrom typing import Optional\nfrom sentence_transformers.evaluation import TripletEvaluator, InformationRetrievalEvaluator\nfrom sentence_transformers.util import cos_sim, mine_hard_negatives\nimport json\nimport time\n#from langchain_community.document_loaders import PyPDFLoader\n#from langchain_text_splitters import RecursiveCharacterTextSplitter","metadata":{"execution":{"iopub.status.busy":"2025-01-14T11:52:08.329664Z","iopub.execute_input":"2025-01-14T11:52:08.330047Z","iopub.status.idle":"2025-01-14T11:52:08.335617Z","shell.execute_reply.started":"2025-01-14T11:52:08.330017Z","shell.execute_reply":"2025-01-14T11:52:08.334469Z"},"id":"3c082570-e6e2-4a6b-b2e4-77e28ff4e54b","trusted":true},"outputs":[],"execution_count":4},{"id":"dfPDT8vUOx3v","cell_type":"code","source":"def load_finetune_dataset(data_file: str, data_config_type: str, train_test_split: Optional[float]=0.8):\n    \"\"\"\n    Load the dataset for finetuning embedding models\n\n    Args:\n        data_file (str): Path to the dataset file.\n        data_config_type (str): Data format type (e.g., \"triplets\", \"pair\").\n        train_test_split(Optional[float]): Ratio of training set. By default, 0.8.\n\n    Return:\n        dataset(dict): dataset with train/validation/test split\n    \"\"\"\n    ds = load_dataset(\"json\", data_files=data_file, split=\"train\")\n    # Rename columns\n    ds = ds.rename_columns({'user_query': 'anchor', 'positive_answer':'positive'})\n    if data_config_type == \"triplets\":\n        ds = ds.rename_column('negative_answer', 'negative')\n    # Add an id column to the dataset\n    ds = ds.add_column(\"id\", range(len(ds)))\n    train_val_split = ds.train_test_split(test_size=1-train_test_split, shuffle=True)\n    val_test_split = train_val_split[\"test\"].train_test_split(test_size=0.5, shuffle=True)\n    dataset = {\n        'train': train_val_split['train'],\n        'validation': val_test_split['train'],\n        'test': val_test_split['test']\n    }\n    return dataset\n\n\ndef get_embedding(text: str, iTokenizer: AutoTokenizer, iModel: AutoModel, iVector: torch.nn.Linear):\n    \"\"\"\n    Obtain the text embedding vectors from the hugging face transformers pipeline.\n\n    Args:\n        text: input text seqeunces\n        iTokenizer: tokenizer\n        iModel: base model\n        iVector: vector linear layer\n\n    Return:\n        query_vectors: embedding vectors\n    \"\"\"\n    with torch.no_grad():\n        input_data = iTokenizer(text, padding=\"longest\", truncation=True, max_length=512, return_tensors=\"pt\")\n        input_data = {k: v.to(\"cpu\") for k, v in input_data.items()}\n        attention_mask = input_data[\"attention_mask\"]\n        last_hidden_state = iModel(**input_data)[0]\n        last_hidden = last_hidden_state.masked_fill(~attention_mask[..., None].bool(), 0.0)\n        query_vectors = last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n        query_vectors = normalize(iVector(query_vectors).cpu().numpy())\n        return query_vectors\n\ndef triplet_evaluator(test_dataset, model):\n    # Evaluate the test dataset\n    test_evaluator = TripletEvaluator(\n                        anchors=test_dataset[\"anchor\"],\n                        positives=test_dataset[\"positive\"],\n                        negatives=test_dataset[\"negative\"],\n                        name=\"triplet_evaluation_test\",\n                        similarity_fn_names=['cosine', 'euclidean']\n                    )\n    results = test_evaluator(model)\n    print(f\"{test_evaluator.primary_metric}: {results[test_evaluator.primary_metric]}\")\n    return results\n\ndef information_retrieval_evaluator(test_dataset, corpus_dataset, model):\n    corpus = dict(zip(corpus_dataset[\"id\"], corpus_dataset[\"positive\"]))\n    queries = dict(zip(test_dataset[\"id\"], test_dataset[\"anchor\"]))\n    relevant_docs = {}\n    for q_id in queries:\n        relevant_docs[q_id] = [q_id]    \n    test_evaluator =  InformationRetrievalEvaluator(\n                        queries=queries,\n                        corpus=corpus,\n                        relevant_docs=relevant_docs,\n                        name=\"eval_finetune_embed\",\n                        score_functions={\"cosine\": cos_sim},\n                    )\n    results = test_evaluator(model)\n    print(f\"{test_evaluator.primary_metric}: {results[test_evaluator.primary_metric]}\")\n    return results","metadata":{"id":"dfPDT8vUOx3v","trusted":true,"execution":{"iopub.status.busy":"2025-01-14T11:51:56.248045Z","iopub.execute_input":"2025-01-14T11:51:56.248841Z","iopub.status.idle":"2025-01-14T11:51:56.260882Z","shell.execute_reply.started":"2025-01-14T11:51:56.248793Z","shell.execute_reply":"2025-01-14T11:51:56.260036Z"}},"outputs":[],"execution_count":3},{"id":"6INo3bZDF7QB","cell_type":"markdown","source":"## Pretrained model\nWe choose `stella_en_400M_v5` model from [MTEB leaderboard](https://huggingface.co/spaces/mteb/leaderboard).","metadata":{"id":"6INo3bZDF7QB"}},{"id":"60c35783-8c8c-48e6-9c48-6af038426916","cell_type":"code","source":"query_prompt_name = \"s2p_query\"\nqueries = [\n    \"What material is the rear wing of the 718 Cayman GT4 RS made of?\",\n    \"What is the unique feature of the Cayenne Turbo GT?\",\n    \"What customization options are available for the Taycan Cross Turismo?\",\n    \"What is the combined CO₂ emissions for the 718 Cayman GTS 4.0?\",\n    \"What is the GTS model?\",\n    \"What makes the Cayenne iconic in its category?\"\n]\n\ndocs = [\n    \"Carbon fiber reinforced plastic (CFRP).\",\n    \"It offers 471 kW (640 PS) and is optimized for high performance.\",\n    \"Exclusive paint finishes, interior trims, and wheel designs.\",\n    \"The 718 Cayman GTS 4.0 produces zero CO₂ emissions because it is fully electric.\",\n    \"GTS model is a great coaching system for MCQ practicing, which G stands for Guardian, T for Teacher and S for Student.\",\n    \"The Cayenne is iconic for being the capital of French Guiana, known for its colonial architecture and a thriving pepper trade.\"\n]\n\n# ！The default dimension is 1024, if you need other dimensions, please clone the model and modify `modules.json` to replace `2_Dense_1024` with another dimension, e.g. `2_Dense_256` or `2_Dense_8192` !\n# on gpu\n# model = SentenceTransformer(\"dunzhang/stella_en_400M_v5\", trust_remote_code=True).cuda()\n# you can also use this model without the features of `use_memory_efficient_attention` and `unpad_inputs`. It can be worked in CPU.\n#model = SentenceTransformer(\n#    \"dunzhang/stella_en_400M_v5\",\n#    trust_remote_code=True,\n#    device=\"cpu\",\n#    config_kwargs={\"use_memory_efficient_attention\": False, \"unpad_inputs\": False}\n#)\n#model = SentenceTransformer(\"mixedbread-ai/mxbai-embed-large-v1\", truncate_dim=512)\n#model = SentenceTransformer('Alibaba-NLP/gte-large-en-v1.5', trust_remote_code=True)\n#model = SentenceTransformer('BAAI/bge-large-en-v1.5', trust_remote_code=True)\nmodel = SentenceTransformer('WhereIsAI/UAE-Large-V1')\nstart_time = time.time()\n#query_embeddings = model.encode(queries, prompt_name=query_prompt_name)\n#query_embeddings = model.encode(queries, prompt_name=\"query\")\nquery_embeddings = model.encode(queries)\ndoc_embeddings = model.encode(docs)\nprint(f\"Inference time: {time.time() - start_time} sec.\")\nprint(query_embeddings.shape, doc_embeddings.shape)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-01-14T12:17:54.554610Z","iopub.execute_input":"2025-01-14T12:17:54.555013Z","iopub.status.idle":"2025-01-14T12:18:33.668854Z","shell.execute_reply.started":"2025-01-14T12:17:54.554983Z","shell.execute_reply":"2025-01-14T12:18:33.667862Z"},"id":"60c35783-8c8c-48e6-9c48-6af038426916","outputId":"52d9228b-e4b1-493e-dff1-727db26d8155","trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a96ae222aa794698a7bdfece8c7f05e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/171 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be3a7f0f9e3e4fb4849af2895332eb75"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/66.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1dfe5b63942a4d39a83319f240e3fac9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21870ac8a3934489a4cdfbb82b3ac177"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/655 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"619f05d4988648ada7bc8f1e50ec5b9f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6203ecb5c0cb403c838db4584dd5c0c3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.24k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12137f1356b64466af4d1bdc942c04fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bddc156cb4824f67baa6e4ee819be032"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ff62cef92504c8e9c3d578dd187ecca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5816aafea8174621a15a7f28bc1cd657"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/297 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4acd25648164449587c73c11ca36d2fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5bee658dc614bada38615e12183726a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37184999a2094abea0f448108f11abbc"}},"metadata":{}},{"name":"stdout","text":"Inference time: 1.7747178077697754 sec.\n(6, 1024) (6, 1024)\n","output_type":"stream"}],"execution_count":27},{"id":"1UzC7R_UR25M","cell_type":"markdown","source":"## Fine-tuned model","metadata":{"id":"1UzC7R_UR25M"}},{"id":"co2NG_uyO5zU","cell_type":"code","source":"!unzip /content/models.zip -d .","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"co2NG_uyO5zU","outputId":"68880697-027f-49b7-88d9-53c1186a18da","_kg_hide-output":true,"trusted":true},"outputs":[],"execution_count":null},{"id":"738efde8-3623-4520-a80e-629a817de437","cell_type":"code","source":"fine_tuned_model_path = \"../input/models/models/stella_en_400M_v5/finetune_triplets_2025-01-02_18-06-49\"\ndense_path = \"../input/models/models/stella_en_400M_v5/finetune_triplets_2025-01-02_18-06-49/2_Dense/model.safetensors\"\nbase_model = AutoModel.from_pretrained(\"dunzhang/stella_en_400M_v5\",\n                            trust_remote_code=True,\n                            device_map='cpu',\n                            use_memory_efficient_attention=False,\n                            unpad_inputs=False)\n\nlora_model = PeftModel.from_pretrained(base_model, fine_tuned_model_path)\n\ntokenizer = AutoTokenizer.from_pretrained(fine_tuned_model_path,\n                            trust_remote_code=True,\n                            device_map='cpu',\n                            use_memory_efficient_attention=False,\n                            unpad_inputs=False)\n\nvector_linear = torch.nn.Linear(in_features=lora_model.config.hidden_size, out_features=1024)\nvector_linear_dict = {\n    k.replace(\"linear.\", \"\"): v for k, v in\n    load_file(dense_path).items()\n}\nvector_linear.load_state_dict(vector_linear_dict)\nvector_linear.to(\"cpu\")\nstart_time = time.time()\nfine_tuned_query_embeddings = get_embedding(queries, tokenizer, lora_model, vector_linear)\nfine_tuned_doc_embeddings = get_embedding(docs, tokenizer, lora_model, vector_linear)\n# fine_tuned_model = SentenceTransformer(\n#     model_path,\n#     device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n#     trust_remote_code=True,\n# )\n# fine_tuned_query_embeddings = fine_tuned_model.encode(queries, prompt_name=query_prompt_name)\n# fine_tuned_doc_embeddings = fine_tuned_model.encode(docs)\nprint(f\"Inference time: {time.time() - start_time} sec.\")\nprint(fine_tuned_query_embeddings.shape, fine_tuned_doc_embeddings.shape)","metadata":{"execution":{"iopub.status.busy":"2025-01-14T11:55:42.364208Z","iopub.execute_input":"2025-01-14T11:55:42.364534Z","iopub.status.idle":"2025-01-14T11:55:57.026102Z","shell.execute_reply.started":"2025-01-14T11:55:42.364510Z","shell.execute_reply":"2025-01-14T11:55:57.025243Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"738efde8-3623-4520-a80e-629a817de437","outputId":"82368beb-279c-4053-e049-4ba72c64af23"},"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at dunzhang/stella_en_400M_v5 were not used when initializing NewModel: ['new.pooler.dense.bias', 'new.pooler.dense.weight']\n- This IS expected if you are initializing NewModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing NewModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:1126: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Inference time: 11.011378526687622 sec.\n(6, 1024) (6, 1024)\n","output_type":"stream"}],"execution_count":8},{"id":"016697cc-69a0-4d2b-b127-0c66930e7a11","cell_type":"code","source":"model = SentenceTransformer(\"../input/models/models/UAE-Large-V1/finetune_triplets_2025-01-12_15-52-10\")\nstart_time = time.time()\n#query_embeddings = model.encode(queries, prompt_name=query_prompt_name)\nquery_embeddings = model.encode(queries)\ndoc_embeddings = model.encode(docs)\nprint(f\"Inference time: {time.time() - start_time} sec.\")\nprint(query_embeddings.shape, doc_embeddings.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T12:19:22.995917Z","iopub.execute_input":"2025-01-14T12:19:22.996251Z","iopub.status.idle":"2025-01-14T12:19:30.145230Z","shell.execute_reply.started":"2025-01-14T12:19:22.996228Z","shell.execute_reply":"2025-01-14T12:19:30.144041Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c2f7cb525684cee8e4cb7d2387dad2b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc81438f070f43c8b731168114bc4e8e"}},"metadata":{}},{"name":"stdout","text":"Inference time: 5.085451602935791 sec.\n(6, 1024) (6, 1024)\n","output_type":"stream"}],"execution_count":28},{"id":"2b0c2706","cell_type":"markdown","source":"## Comparison between fine-tuned model and pre-trained model\nIssues of pretrained model:\n1. positive example sentence pairs but low similariy\n2. negative example sentence pairs but high similariy\nAfter finetuning, the fine-tuned model can improve the performance.","metadata":{"id":"2b0c2706"}},{"id":"46386b26","cell_type":"markdown","source":"### Case 1: Positive answer but low similarity score\n#### Pretrained model","metadata":{"id":"46386b26"}},{"id":"3f4dacee","cell_type":"code","source":"for i in range(3):\n    similarities = model.similarity(query_embeddings[i], doc_embeddings[i])\n    print(f\"Uesr Query: {queries[i]}\")\n    print(f\"Answer: {docs[i]}\")\n    print(f\"Similarity score: {similarities.data.cpu().numpy()[0][0]} \\n\\n\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3f4dacee","outputId":"5db64338-f007-41ab-bd64-1c3e85f4cbeb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Uesr Query: What material is the rear wing of the 718 Cayman GT4 RS made of?\n","Answer: Carbon fiber reinforced plastic (CFRP).\n","Similarity score: 0.5579702258110046 \n","\n","\n","Uesr Query: What is the unique feature of the Cayenne Turbo GT?\n","Answer: It offers 471 kW (640 PS) and is optimized for high performance.\n","Similarity score: 0.41068923473358154 \n","\n","\n","Uesr Query: What customization options are available for the Taycan Cross Turismo?\n","Answer: Exclusive paint finishes, interior trims, and wheel designs.\n","Similarity score: 0.5345100164413452 \n","\n","\n"]}],"execution_count":10},{"id":"9f51e83a","cell_type":"markdown","source":"#### Fine-tuned model","metadata":{"id":"9f51e83a"}},{"id":"6i0cRt2zQT4q","cell_type":"code","source":"fine_tuned_similarities = fine_tuned_query_embeddings @ fine_tuned_doc_embeddings.T","metadata":{"id":"6i0cRt2zQT4q"},"outputs":[],"execution_count":17},{"id":"40029c19","cell_type":"code","source":"for i in range(3):\n    # similarities = model.similarity(fine_tuned_query_embeddings[i], fine_tuned_doc_embeddings[i])\n    print(f\"Uesr Query: {queries[i]}\")\n    print(f\"Answer: {docs[i]}\")\n    # print(f\"Similarity score: {similarities.data.cpu().numpy()[0][0]} \\n\\n\")\n    print(f\"Similarity score: {fine_tuned_similarities[i, i]} \\n\\n\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"40029c19","outputId":"215f1b4a-161d-4f41-870f-25d434766ce7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Uesr Query: What material is the rear wing of the 718 Cayman GT4 RS made of?\n","Answer: Carbon fiber reinforced plastic (CFRP).\n","Similarity score: 0.8132065534591675 \n","\n","\n","Uesr Query: What is the unique feature of the Cayenne Turbo GT?\n","Answer: It offers 471 kW (640 PS) and is optimized for high performance.\n","Similarity score: 0.908795177936554 \n","\n","\n","Uesr Query: What customization options are available for the Taycan Cross Turismo?\n","Answer: Exclusive paint finishes, interior trims, and wheel designs.\n","Similarity score: 0.7588391900062561 \n","\n","\n"]}],"execution_count":18},{"id":"e64451f5","cell_type":"markdown","source":"### Case 2: negative answer but high similariy\n#### Pretrained model","metadata":{"id":"e64451f5"}},{"id":"b9b72957","cell_type":"code","source":"for i in range(3, 6):\n    similarities = model.similarity(query_embeddings[i], doc_embeddings[i])\n    print(f\"Uesr Query: {queries[i]}\")\n    print(f\"Answer: {docs[i]}\")\n    print(f\"Similarity score: {similarities.data.cpu().numpy()[0][0]} \\n\\n\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b9b72957","outputId":"50df6b5e-e255-4ac7-ccbc-23e46e5b97a5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Uesr Query: What is the combined CO₂ emissions for the 718 Cayman GTS 4.0?\n","Answer: The 718 Cayman GTS 4.0 produces zero CO₂ emissions because it is fully electric.\n","Similarity score: 0.8328068256378174 \n","\n","\n","Uesr Query: What is the GTS model?\n","Answer: GTS model is a great coaching system for MCQ practicing, which G stands for Guardian, T for Teacher and S for Student.\n","Similarity score: 0.7170218229293823 \n","\n","\n","Uesr Query: What makes the Cayenne iconic in its category?\n","Answer: The Cayenne is iconic for being the capital of French Guiana, known for its colonial architecture and a thriving pepper trade.\n","Similarity score: 0.6261855363845825 \n","\n","\n"]}],"execution_count":19},{"id":"27cf07f0","cell_type":"markdown","source":"#### Fine-tuned model","metadata":{"id":"27cf07f0"}},{"id":"dbfa61d2-b187-467e-ab11-e628e871f929","cell_type":"code","source":"for i in range(3, 6):\n    # similarities = fine_tuned_model.similarity(fine_tuned_query_embeddings[i], fine_tuned_doc_embeddings[i])\n    print(f\"Uesr Query: {queries[i]}\")\n    print(f\"Answer: {docs[i]}\")\n    # print(f\"Similarity score: {similarities.data.cpu().numpy()[0][0]} \\n\\n\")\n    print(f\"Similarity score: {fine_tuned_similarities[i, i]} \\n\\n\")","metadata":{"execution":{"iopub.execute_input":"2025-01-03T00:06:35.308026Z","iopub.status.busy":"2025-01-03T00:06:35.307706Z","iopub.status.idle":"2025-01-03T00:06:35.316182Z","shell.execute_reply":"2025-01-03T00:06:35.315303Z","shell.execute_reply.started":"2025-01-03T00:06:35.307998Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"dbfa61d2-b187-467e-ab11-e628e871f929","outputId":"aa3d9979-a9da-499c-b7db-468b235f276a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Uesr Query: What is the combined CO₂ emissions for the 718 Cayman GTS 4.0?\n","Answer: The 718 Cayman GTS 4.0 produces zero CO₂ emissions because it is fully electric.\n","Similarity score: 0.15441642701625824 \n","\n","\n","Uesr Query: What is the GTS model?\n","Answer: GTS model is a great coaching system for MCQ practicing, which G stands for Guardian, T for Teacher and S for Student.\n","Similarity score: 0.35840776562690735 \n","\n","\n","Uesr Query: What makes the Cayenne iconic in its category?\n","Answer: The Cayenne is iconic for being the capital of French Guiana, known for its colonial architecture and a thriving pepper trade.\n","Similarity score: 0.18086686730384827 \n","\n","\n"]}],"execution_count":20},{"id":"gy8KXAbEQ9gV","cell_type":"markdown","source":"## Comparison with different embedding models\nThe embedding models are chosen from [SBERT library](https://sbert.net/) and two embedding model benchmarks, i.e. [Huggingface MTEB](https://huggingface.co/spaces/mteb/leaderboard) and [Crossing Minds ICLERB](https://www.crossingminds.com/company-resources/iclerb).\n\nThe chosen models are listed as follows:\n- [dunzhang/stella_en_400M_v5](https://huggingface.co/dunzhang/stella_en_400M_v5)\n- [nomic-ai/nomic-embed-text-v1.5](https://huggingface.co/nomic-ai/nomic-embed-text-v1.5)\n- [all-mpnet-base-v2](https://www.sbert.net/docs/sentence_transformer/pretrained_models.html)\n- [BAAI/bge-large-en-v1.5](https://huggingface.co/BAAI/bge-large-en-v1.5)\n- [BAAI/bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5)\n- [mixedbread-ai/mxbai-embed-large-v1](https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1)\n- [Alibaba-NLP/gte-large-en-v1.5](https://huggingface.co/Alibaba-NLP/gte-large-en-v1.5)\n- [WhereIsAI/UAE-Large-V1](https://huggingface.co/WhereIsAI/UAE-Large-V1)","metadata":{"id":"gy8KXAbEQ9gV"}},{"id":"qqb6dOdMVAiX","cell_type":"markdown","source":"### Data type: triplets {query, positive, negative}\n","metadata":{"id":"qqb6dOdMVAiX"}},{"id":"ea0f832e-0a90-413b-834b-6a7f8fb1d700","cell_type":"code","source":"print(os.listdir(\"../input\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T21:28:15.767364Z","iopub.execute_input":"2025-01-12T21:28:15.767658Z","iopub.status.idle":"2025-01-12T21:28:15.774962Z","shell.execute_reply.started":"2025-01-12T21:28:15.767635Z","shell.execute_reply":"2025-01-12T21:28:15.774244Z"}},"outputs":[{"name":"stdout","text":"['models', 'qa_pairs_pos_only.json', 'qa_pairs_pos_and_neg.json']\n","output_type":"stream"}],"execution_count":8},{"id":"9df2159b-b715-441b-8ecd-7255792ec162","cell_type":"code","source":"results = dict()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T10:19:57.564619Z","iopub.execute_input":"2025-01-14T10:19:57.564966Z","iopub.status.idle":"2025-01-14T10:19:57.593897Z","shell.execute_reply.started":"2025-01-14T10:19:57.564941Z","shell.execute_reply":"2025-01-14T10:19:57.592811Z"}},"outputs":[],"execution_count":6},{"id":"BAzK0aMfTUG0","cell_type":"code","source":"# Prepare test dataset based on the data configuration format\ndata_config_type = \"triplets\"\ndata_file = \"../input/qa_pairs_pos_and_neg.json\"\ndataset = load_finetune_dataset(data_file, data_config_type)\ntrain_dataset = dataset[\"train\"]\neval_dataset = dataset[\"validation\"]\ntest_dataset = dataset[\"test\"]","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["c65c761364a64f8d9b387999a2fe9234","9dbee88ce7e7411aa3c75fcbae8fd657","a365f0f61c224a189e89a711328d06be","77314fd9e9664d01bae0684311f04682","1f39ad7d421a412db0a35471aaab9f31","d5c66c125d384526b517cd35144a79a2","b171bd2024884b7e97d61bb70d200b98","ba9d95dc32a149f4b3b942410c7bb134","1365760bf0914b9fb613c122566a157e","731b45ec695f4eaaa2d917e3817ef3a7","3292e64d80d64a999833f7b8d3ed04be"]},"id":"BAzK0aMfTUG0","outputId":"38ace573-a458-4bf5-8189-5eae23a80aae","trusted":true,"execution":{"iopub.status.busy":"2025-01-14T11:34:41.056657Z","iopub.execute_input":"2025-01-14T11:34:41.056946Z","iopub.status.idle":"2025-01-14T11:34:41.456601Z","shell.execute_reply.started":"2025-01-14T11:34:41.056925Z","shell.execute_reply":"2025-01-14T11:34:41.455755Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5da06792b6c04806b9cf049b92da5343"}},"metadata":{}}],"execution_count":5},{"id":"6b3bc604-9b39-4991-8e12-2349c1dde46e","cell_type":"code","source":"base = SentenceTransformer(\"nomic-ai/nomic-embed-text-v1.5\", trust_remote_code=True).cuda()\nbase_results = triplet_evaluator(test_dataset, base)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T11:35:09.344246Z","iopub.execute_input":"2025-01-14T11:35:09.344536Z","iopub.status.idle":"2025-01-14T11:35:29.607564Z","shell.execute_reply.started":"2025-01-14T11:35:09.344515Z","shell.execute_reply":"2025-01-14T11:35:29.606678Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"configuration_hf_nomic_bert.py:   0%|          | 0.00/1.96k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bfea0d1ef4944faf95d7625fb39a6390"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/nomic-ai/nomic-bert-2048:\n- configuration_hf_nomic_bert.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modeling_hf_nomic_bert.py:   0%|          | 0.00/95.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96bde60d2fd94f6db5f18180dca04482"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/nomic-ai/nomic-bert-2048:\n- modeling_hf_nomic_bert.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/547M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ecbc9487e6349b8a73871797784ac49"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.19k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53dd213320904872a5caaf00c3a5b5a9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38c13ac12d84475bb4e78326e1d55ab2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"417690ed372147f09ad52cf0d7c1fd87"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/695 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf1cdf8e27d244e6b809e51bea5f8bd8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/286 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ef8e12e8444436d9d79992e9b95d7d6"}},"metadata":{}},{"name":"stdout","text":"triplet_evaluation_test_max_accuracy: 0.7289156626506024\n","output_type":"stream"}],"execution_count":7},{"id":"cMSjCe7iQ5Cg","cell_type":"code","source":"model_path = \"../input/models/models/UAE-Large-V1/finetune_triplets_2025-01-12_15-52-10\"\nfinetune = SentenceTransformer(model_path, device=\"cuda\" if torch.cuda.is_available() else \"cpu\", trust_remote_code=True)\nfinetune_results = triplet_evaluator(test_dataset, finetune)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cMSjCe7iQ5Cg","outputId":"4f77a752-ea35-47c5-aeb8-1c672cbc95c8","trusted":true,"execution":{"iopub.status.busy":"2025-01-14T10:30:32.068689Z","iopub.execute_input":"2025-01-14T10:30:32.069080Z","iopub.status.idle":"2025-01-14T10:30:38.797340Z","shell.execute_reply.started":"2025-01-14T10:30:32.069048Z","shell.execute_reply":"2025-01-14T10:30:38.796460Z"}},"outputs":[{"name":"stdout","text":"triplet_evaluation_test_max_accuracy: 0.9939759036144579\n","output_type":"stream"}],"execution_count":36},{"id":"bf09a343-aca8-4262-b947-fee4ba7ac918","cell_type":"code","source":"results[\"UAE\"] = {}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T10:30:41.260206Z","iopub.execute_input":"2025-01-14T10:30:41.260531Z","iopub.status.idle":"2025-01-14T10:30:41.264611Z","shell.execute_reply.started":"2025-01-14T10:30:41.260504Z","shell.execute_reply":"2025-01-14T10:30:41.263649Z"}},"outputs":[],"execution_count":37},{"id":"6e536dd3-f587-445a-9610-9c963f8723fe","cell_type":"code","source":"results[\"UAE\"][\"triplets\"] = {}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T10:30:42.279266Z","iopub.execute_input":"2025-01-14T10:30:42.279631Z","iopub.status.idle":"2025-01-14T10:30:42.283885Z","shell.execute_reply.started":"2025-01-14T10:30:42.279600Z","shell.execute_reply":"2025-01-14T10:30:42.282856Z"}},"outputs":[],"execution_count":38},{"id":"e7dde130-f606-45d3-b9bd-bcee15c88754","cell_type":"code","source":"results[\"UAE\"][\"triplets\"] = {\"base_model\": base_results, \"fine_tine\": finetune_results}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T10:30:44.343917Z","iopub.execute_input":"2025-01-14T10:30:44.344234Z","iopub.status.idle":"2025-01-14T10:30:44.348612Z","shell.execute_reply.started":"2025-01-14T10:30:44.344210Z","shell.execute_reply":"2025-01-14T10:30:44.347487Z"}},"outputs":[],"execution_count":39},{"id":"52f9310a-611d-40f3-8f1d-0c8c94089a00","cell_type":"code","source":"data_config_type = \"pair\"\ndata_file = \"../input/qa_pairs_pos_only.json\"\ndataset = load_finetune_dataset(data_file, data_config_type)\ntrain_dataset = dataset[\"train\"]\neval_dataset = dataset[\"validation\"]\ntest_dataset = dataset[\"test\"]\ncorpus_dataset = concatenate_datasets([train_dataset, eval_dataset, test_dataset])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T11:35:38.710416Z","iopub.execute_input":"2025-01-14T11:35:38.710731Z","iopub.status.idle":"2025-01-14T11:35:39.008062Z","shell.execute_reply.started":"2025-01-14T11:35:38.710709Z","shell.execute_reply":"2025-01-14T11:35:39.007055Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"922b9163476e4999b4642a996e9cea04"}},"metadata":{}}],"execution_count":9},{"id":"cf3b2d69-57f8-44d5-9d6d-e883a8a6a219","cell_type":"code","source":"base_results = information_retrieval_evaluator(test_dataset, corpus_dataset, base)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T11:35:40.012983Z","iopub.execute_input":"2025-01-14T11:35:40.013316Z","iopub.status.idle":"2025-01-14T11:35:43.843871Z","shell.execute_reply.started":"2025-01-14T11:35:40.013289Z","shell.execute_reply":"2025-01-14T11:35:43.842911Z"}},"outputs":[{"name":"stdout","text":"eval_finetune_embed_cosine_ndcg@10: 0.6721534613276363\n","output_type":"stream"}],"execution_count":10},{"id":"0ab1fa1e-5df9-4d44-ac8a-5de1ad3d1f05","cell_type":"code","source":"model_path = \"../input/models/models/UAE-Large-V1/finetune_pair_2025-01-12_16-05-51\"\nfinetune = SentenceTransformer(model_path, device=\"cuda\" if torch.cuda.is_available() else \"cpu\", trust_remote_code=True)\nfinetune_results = information_retrieval_evaluator(test_dataset, corpus_dataset, finetune)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T10:31:05.752202Z","iopub.execute_input":"2025-01-14T10:31:05.752537Z","iopub.status.idle":"2025-01-14T10:31:23.467615Z","shell.execute_reply.started":"2025-01-14T10:31:05.752510Z","shell.execute_reply":"2025-01-14T10:31:23.466596Z"}},"outputs":[{"name":"stdout","text":"eval_finetune_embed_cosine_ndcg@10: 0.7118463341691333\n","output_type":"stream"}],"execution_count":42},{"id":"067d9d9a-c0af-4981-bf0e-770fa8c12333","cell_type":"code","source":"results[\"UAE\"][\"pair\"] = {}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T10:31:27.751247Z","iopub.execute_input":"2025-01-14T10:31:27.751615Z","iopub.status.idle":"2025-01-14T10:31:27.756400Z","shell.execute_reply.started":"2025-01-14T10:31:27.751587Z","shell.execute_reply":"2025-01-14T10:31:27.755116Z"}},"outputs":[],"execution_count":43},{"id":"260544c8-fcd1-4df6-9c52-09e5155a7af0","cell_type":"code","source":"results[\"UAE\"][\"pair\"] = {\"base_model\": base_results, \"fine_tine\": finetune_results}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T10:31:28.640690Z","iopub.execute_input":"2025-01-14T10:31:28.641036Z","iopub.status.idle":"2025-01-14T10:31:28.645073Z","shell.execute_reply.started":"2025-01-14T10:31:28.641012Z","shell.execute_reply":"2025-01-14T10:31:28.644059Z"}},"outputs":[],"execution_count":44},{"id":"79428778-d81e-4a3f-867e-bf33eba98663","cell_type":"code","source":"with open('/kaggle/working/bge_large_and_allmpnet_and_UAE.json', 'w') as f:\n    json.dump(results, f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T10:31:33.046680Z","iopub.execute_input":"2025-01-14T10:31:33.047084Z","iopub.status.idle":"2025-01-14T10:31:33.052619Z","shell.execute_reply.started":"2025-01-14T10:31:33.047053Z","shell.execute_reply":"2025-01-14T10:31:33.051699Z"}},"outputs":[],"execution_count":46}]}